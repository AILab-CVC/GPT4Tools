{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52bc9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-16 14:20:11,720] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "from gpt4tools.data.conversation import SeparatorStyle, get_conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46111ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"lmsys/vicuna-7b-v1.5\"\n",
    "CACHE_DIR=\"/data01/rayyang/GPT4Tools/cache\"\n",
    "model_max_length=2048\n",
    "lora_path=\"/data01/rayyang/GPT4Tools/output_vicuna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b727ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7e9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752c2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from /data01/rayyang/GPT4Tools/output_vicuna\n",
      "Merging weights\n",
      "Convert to FP16...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loading LoRA weights from {lora_path}\")\n",
    "model = PeftModel.from_pretrained(original_model, lora_path)\n",
    "print(f\"Merging weights\")\n",
    "model = model.merge_and_unload()\n",
    "print('Convert to FP16...')\n",
    "model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e2f45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth explanations and discussions. It generates human-like text and uses tools to indirectly understand images. When referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses tools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description, but tools must be used for subsequent tasks.\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "GPT4Tools has access to the following tools:\n",
      "\n",
      "> Edge Detection On Image: useful when you want to detect the edge of the image. like: detect the edges of this image, or canny detection on image, or perform edge detection on this image, or detect the canny image of this image. The input to this tool should be a string, representing the image_path\n",
      "> Generate Image Condition On Canny Image: useful when you want to generate a new real image from both the user description and a canny image. like: generate a real image of a object or something from this canny image, or generate a new real image of a object or something from this edge image. The input to this tool should be a comma separated string of two, representing the image_path and the user description.\n",
      "> Remove Something From The Photo: useful when you want to remove and object or something from the photo from its description or location. The input to this tool should be a comma separated string of two, representing the image_path and the object need to be removed.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Edge Detection On Image, Generate Image Condition On Canny Image, Hed Detection On Image, Segment the given object]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Follow file name rules and do not fake non-existent file names. Remember to provide the image file name loyally from the last tool observation.\n",
      "\n",
      "Previous conversation:\n",
      "\n",
      "Human: Provide an image named image/alzkcuai.png. Description: A person is lying on the grass. Understand the image using tools.\n",
      "AI: Received.\n",
      "\n",
      "New input: Remove the person in the image. \n",
      "GPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the conversation are only visible to GPT4Tools. When answering human questions, repeat important information. Let's think step by step.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "GPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth explanations and discussions. It generates human-like text and uses tools to indirectly understand images. When referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses tools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description, but tools must be used for subsequent tasks.\n",
    "TOOLS:\n",
    "------\n",
    "\n",
    "GPT4Tools has access to the following tools:\n",
    "\n",
    "> Edge Detection On Image: useful when you want to detect the edge of the image. like: detect the edges of this image, or canny detection on image, or perform edge detection on this image, or detect the canny image of this image. The input to this tool should be a string, representing the image_path\n",
    "> Generate Image Condition On Canny Image: useful when you want to generate a new real image from both the user description and a canny image. like: generate a real image of a object or something from this canny image, or generate a new real image of a object or something from this edge image. The input to this tool should be a comma separated string of two, representing the image_path and the user description.\n",
    "> Remove Something From The Photo: useful when you want to remove and object or something from the photo from its description or location. The input to this tool should be a comma separated string of two, representing the image_path and the object need to be removed.\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [Edge Detection On Image, Generate Image Condition On Canny Image, Hed Detection On Image, Segment the given object]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "AI: [your response here]\n",
    "```\n",
    "\n",
    "Follow file name rules and do not fake non-existent file names. Remember to provide the image file name loyally from the last tool observation.\n",
    "\n",
    "Previous conversation:\n",
    "\n",
    "Human: Provide an image named image/alzkcuai.png. Description: A person is lying on the grass. Understand the image using tools.\n",
    "AI: Received.\n",
    "\n",
    "New input: Remove the person in the image. \n",
    "GPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the conversation are only visible to GPT4Tools. When answering human questions, repeat important information. Let's think step by step.\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb05384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: \n",
      "GPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth explanations and discussions. It generates human-like text and uses tools to indirectly understand images. When referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses tools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description, but tools must be used for subsequent tasks.\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "GPT4Tools has access to the following tools:\n",
      "\n",
      "> Edge Detection On Image: useful when you want to detect the edge of the image. like: detect the edges of this image, or canny detection on image, or perform edge detection on this image, or detect the canny image of this image. The input to this tool should be a string, representing the image_path\n",
      "> Generate Image Condition On Canny Image: useful when you want to generate a new real image from both the user description and a canny image. like: generate a real image of a object or something from this canny image, or generate a new real image of a object or something from this edge image. The input to this tool should be a comma separated string of two, representing the image_path and the user description.\n",
      "> Remove Something From The Photo: useful when you want to remove and object or something from the photo from its description or location. The input to this tool should be a comma separated string of two, representing the image_path and the object need to be removed.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Edge Detection On Image, Generate Image Condition On Canny Image, Hed Detection On Image, Segment the given object]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Follow file name rules and do not fake non-existent file names. Remember to provide the image file name loyally from the last tool observation.\n",
      "\n",
      "Previous conversation:\n",
      "\n",
      "Human: Provide an image named image/alzkcuai.png. Description: A person is lying on the grass. Understand the image using tools.\n",
      "AI: Received.\n",
      "\n",
      "New input: Remove the person in the image. \n",
      "GPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the conversation are only visible to GPT4Tools. When answering human questions, repeat important information. Let's think step by step.\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "conv = get_conv_template(\"gpt4tools_vicuna_v1.1\")\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "my_prompt = conv.get_prompt()\n",
    "print(my_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf89c2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Remove Something From The Photo\n",
      "Action Input: image/alzkcuai.png, person\n",
      "Observation:\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([my_prompt], return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "output_ids = output_ids[0][len(inputs[\"input_ids\"][0]) :]\n",
    "outputs = tokenizer.decode(\n",
    "    output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    ")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4787e055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"USER: \\nGPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth explanations and discussions. It generates human-like text and uses tools to indirectly understand images. When referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses tools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description, but tools must be used for subsequent tasks.\\nTOOLS:\\n------\\n\\nGPT4Tools has access to the following tools:\\n\\n> Edge Detection On Image: useful when you want to detect the edge of the image. like: detect the edges of this image, or canny detection on image, or perform edge detection on this image, or detect the canny image of this image. The input to this tool should be a string, representing the image_path\\n> Generate Image Condition On Canny Image: useful when you want to generate a new real image from both the user description and a canny image. like: generate a real image of a object or something from this canny image, or generate a new real image of a object or something from this edge image. The input to this tool should be a comma separated string of two, representing the image_path and the user description.\\n> Remove Something From The Photo: useful when you want to remove and object or something from the photo from its description or location. The input to this tool should be a comma separated string of two, representing the image_path and the object need to be removed.\\n\\nTo use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [Edge Detection On Image, Generate Image Condition On Canny Image, Hed Detection On Image, Segment the given object]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nAI: [your response here]\\n```\\n\\nFollow file name rules and do not fake non-existent file names. Remember to provide the image file name loyally from the last tool observation.\\n\\nPrevious conversation:\\n\\nHuman: Provide an image named image/alzkcuai.png. Description: A person is lying on the grass. Understand the image using tools.\\nAI: Received.\\n\\nNew input: Remove the person in the image. \\nGPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the conversation are only visible to GPT4Tools. When answering human questions, repeat important information. Let's think step by step.\\nASSISTANT:\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7efa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
